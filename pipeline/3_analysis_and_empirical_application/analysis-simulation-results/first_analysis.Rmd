---
title: "First analysis of simulation results"
author: "Jasper Ginn"
date: "3/31/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, error = FALSE)
library(sleepsimReval)
library(sleepsimR)
library(dplyr)
# Read results
rm(list=ls())
res <- readRDS("res_parsed.rds")
# Read scenarios
scen <- read.csv("../make_scenarios/scenarios_subs.csv.gz",
                 stringsAsFactors = FALSE)
scen2 <- read.csv("../make_scenarios/scenarios.csv.gz",
                 stringsAsFactors = FALSE)
# Bind rows
scen <- bind_rows(scen, scen2)
# Real values (means)
rv <- data.frame(
  "var" = c("EEG_mean_beta_mean_state1", "EEG_mean_beta_mean_state2",
            "EEG_mean_beta_mean_state3", "EOG_median_theta_mean_state1",
            "EOG_median_theta_mean_state2", "EOG_median_theta_mean_state3",
            "EOG_min_beta_mean_state1", "EOG_min_beta_mean_state2",
            "EOG_min_beta_mean_state3"),
  "val" = c(-0.36, -0.6, 0.7, 1.01, -1.31, -.24, .75, -1.31, 0.005),
  stringsAsFactors = FALSE
)
# Libraries
library(magrittr)
library(dplyr)
library(tidyr)
```

# Introduction

This is a first analysis of the simulation results. I ran $48$ iterations for each scenario. More results are expected in the coming weeks.  

## State-dependent emission distribution means

### bias

```{r}
vmean <- res %>%
  get("emiss_mu_bar") %>%
  bind_rows() %>%
  select_at(vars(contains("mean_state")))
vmean$iteration_id <- res %>%
  get("iteration_uid") %>%
  unname() %>%
  unlist()
vmean_orig <- vmean
# Merge
vmean <- vmean %>%
  left_join(scen %>%
              select(iteration_id,
                     scenario_id))
vmean <- vmean %>%
  # Gather
  gather(var, val, -iteration_id, -scenario_id)
# Summarize by group
library(purrr)
op <- vmean %>%
  split(list(vmean$scenario_id, vmean$var)) %>%
  map(function(x) {
    vari <- unique(x$var)
    gt <- rv %>% filter(var == vari) %>%
      select(val) %>% pull()
    val <- x$val
    bb <- unname(bias(gt, val))
    ese <- unname(emperical_SE(val))
    MSE <- MSE(val, gt)
    return(
      data.frame(
        "bias" = (bb[1]/gt) * 100,
        "emp_se" = ese[1],
        "MSE" = MSE,
        "MCMC_ERR" = bb[2],
        "MCMC_ERR_P" = abs(bb[2] / gt) * 100,
        "scenario_id" = x$scenario_id[1],
        "var" = vari,
        stringsAsFactors = FALSE)
    )
  }) %>%
  do.call(rbind.data.frame, .)
# Get scenarios
op2 <- op %>%
  left_join(., scen %>%
              select(scenario_id, n, n_t, zeta, Q) %>%
              distinct()) 
# Estimate of variance parameter estimates
varest <- op2 %>% select(-bias, -MSE, -scenario_id) %>% 
  #filter(zeta == 2 & Q == 0.4 & n == 10 & n_t == 400) %>% 
  mutate(var_est = emp_se ^ 2) %>%
  left_join(rv) %>%
  mutate(mcerr = sqrt(var_est / 250),
         mcerr_p = (mcerr / val) * 100) %>%
  arrange(desc(mcerr))

library(ggplot2)
ggplot(op2 %>%
         filter(Q == 0.2), aes(x=n, y=bias, linetype=as.factor(n_t), color=as.factor(zeta))) +
  geom_point(size=1.7) +
  geom_line() +
  geom_hline(yintercept=0, color = "brown") +
  facet_wrap(". ~ var", scales="free_y") +
  theme_bw()
```

* Major trend: adding more subjects most important for lower bias in estimates. Trend is stable across TPM variance.
* Can deal with a reasonable amount of variance (1 Zscore). But more than that leads to high bias of estimates. 
* High-variance setting (purple line) does not improve by adding multiple subjects. Too much variability between means. What does this setting look like?

```{r}
d <- simulate_dataset(80, 1600, 2, 0.4, 444)
df <- as.data.frame(d$obs)
colnames(df) <- c("pid", "EEG_beta", "EOG_median_theta", "EOG_mean_theta")
df$states <- d$states[,2]
ggplot(df %>% gather(var, val, -pid, -states), aes(x=val, fill = as.factor(states))) +
  geom_density(alpha=0.4) +
  facet_wrap(". ~ var")
```

* State 3 has worst estimation. This is not strange if you consider that it is wedged in between the other states. At lower between-subject variances, the bias is acceptable. In particular the estimation of REM in EOG_min_beta is severely biased downwards.

No matter how many observations are used per person, when we look at the boxplots of the bias estimates, it seems that $n=40$ is the minimum number of subjects required to remove the downwards bias for most of the estimates.

```{r}
# Plot distribution
op3 <- vmean %>%
  left_join(scen %>%
              select(scenario_id, n, n_t, zeta, Q) %>%
              distinct()) %>%
  left_join(rv, by ="var") %>%
  mutate(pbias = ((val.x - val.y)/val.y) * 100)
ggplot(op3 %>%
         filter(Q==0.4, zeta==1, n_t == 800), aes(x=as.factor(n), y=pbias)) +
    geom_boxplot() +
    geom_hline(yintercept=0, color="brown", linetype="dashed") +
    facet_wrap(". ~ var", scales = "free_y")
```

### Emperical SE

From the emperical SE below, it is clear that the estimates typically become more precise as $n$ increases. What is interesting here is that increasing $n_t$ does not seem to help at all in obtaining more efficient estimates; they only help in terms of obtaining less biased estimates.

```{r}
ggplot(op2 %>%
         filter(Q == 0.1), aes(x=n, y=emp_se, linetype=as.factor(n_t), color=as.factor(zeta))) +
  geom_point(size=1.7) +
  geom_line() +
  geom_hline(yintercept=0, color = "brown") +
  facet_wrap(". ~ var", scales="free_y") +
  theme_bw()
```

Perhaps unexpectedly, the relative preservation of the ordering by between-subject variance indicates that lower between-subject variance leads to more precise estimates. For the setting of $zeta=0.25$ one needs only $n=40$ for good estimates. The wdith between settings decreases as $n$ increases.

### MSE

We see the same trends as above. 

```{r}
ggplot(op2 %>%
         filter(Q == 0.4), aes(x=n, y=sqrt(MSE), linetype=as.factor(n_t), color=as.factor(zeta))) +
  geom_point(size=1.7) +
  geom_line() +
  geom_hline(yintercept=0, color = "brown") +
  facet_wrap(". ~ var", scales="free_y") +
  theme_bw()
```

This suggests that the consistently biased estimates will likely experience reduced coverage as the number of subjects increases.

### Coverage

```{r}
# Emission distribution means
em_dist_means <- lapply(res, function(x) {
  ci <- x$credible_intervals$emiss_mu_bar
  nams <- names(ci)
  for(idx in seq_along(nams)) {
    names(ci[[idx]]) <- paste0("mean_",
                               c("state1_lower", "state1_upper",
                                 "state2_lower", "state2_upper",
                                 "state3_lower", "state3_upper"))
  }
  y <- do.call(cbind.data.frame, ci)
  colnames(y) <- gsub("\\.", "_", colnames(y))
  y
})
# Bind
em_dist_means <- do.call(rbind.data.frame, em_dist_means)
em_dist_means$iteration_id <- vmean_orig$iteration_id
em_dist_means <- em_dist_means %>%
  left_join(scen %>%
              select(scenario_id, iteration_id),
            by = "iteration_id")
# Get scenarios
vmean <- vmean_orig %>%
  left_join(scen %>%
              select(scenario_id, iteration_id),
            by = "iteration_id") %>%
  gather(var, val, -iteration_id, -scenario_id)
# Split by scenario
vmean_split <- split(vmean, list(vmean$scenario_id, vmean$var))
# Compute coverage
out_coverage <- vmean_split %>%
  map(., function(x) {
    ss <- em_dist_means %>%
      filter(scenario_id == x$scenario_id[1]) %>%
      select(starts_with(x$var[1]))
    ci <- lapply(1:nrow(ss), function(z) unname(unlist(ss[z,])))
    gt <- rv %>%
      filter(var == x$var[1]) %>%
      select(val) %>%
      pull()
    covv <- unname(coverage(ci, gt))
    data.frame(
      iteration_id = x$iteration_id[1],
      scenario_id = x$scenario_id[1],
      var = x$var[1],
      coverage = covv[1]
    )
  })
out_coverage <- do.call(rbind.data.frame, out_coverage)
# Combine with scenarios
out_coverage <- out_coverage %>%
  left_join(scen %>%
              select(scenario_id, n, n_t, Q, zeta) %>%
              distinct(),
            by = "scenario_id")
ggplot(out_coverage %>%
         filter(Q==0.4), aes(x=n, y=coverage, linetype=as.factor(n_t), color=as.factor(zeta))) +
  geom_point(size=1.7) +
  geom_line() +
  geom_hline(yintercept=0.98, color = "brown", size=1.8) +
  geom_hline(yintercept=0.90, color = "brown", size=1.8) +
  facet_wrap(". ~ var")
```

The coverage of the parameter estimates is decent, even in the case of parameters that display quite some bias and for cases in which the between-subject variance was very high ($zeta=2$). The conclusion here seems to be that even moderate amounts of between-subject variance may still work as long as the number of data points for each individual are high.

## Random effects

### Bias

Two things really stand out here. 

1. High between-person variance is much easier to detect than low between-person variance.
2. The ability of the model to estimate BS variance is dependent on the number of subjects, not the number of time steps. But that makes sense.
3. Still, timesteps of $n_t=1.600$ tend to yield less biased estimates than $n_t=400$. Not clear if this is worth the additional effort to collect more values.

```{r}
# Variance
var2 <- res %>%
  get("emiss_varmu_bar") %>%
  bind_rows() %>%
  select_at(vars(contains("mean_state")))
var2$iteration_id <- res %>%
  get("iteration_uid") %>%
  unname() %>%
  unlist() 
varm_orig <- var2
# Merge
var2 <- var2 %>%
  left_join(scen %>%
              select(iteration_id,
                     scenario_id))
var2 <- var2 %>%
  # Gather
  gather(var, val, -iteration_id, -scenario_id)
# Summarize by group
library(purrr)
op <- var2 %>%
  split(list(var2$scenario_id, var2$var))
op <- op %>%
  map(function(x) {
    vari <- unique(x$var)
    gt <- scen %>% filter(iteration_id == x$iteration_id[1]) %>%
      select(zeta) %>% pull()
    val <- x$val
    bb <- unname(bias(gt, val))
    ese <- unname(emperical_SE(val))
    MSE <- MSE(val, gt)
    return(
      data.frame(
        "bias" = (bb[1] / gt) * 100,
        "emp_se" = ese[1],
        "MSE" = MSE,
        "MCMC_ERR" = bb[2],
        "MCMC_ERR_p" = (bb[2] / b[1]) * 100,
        "scenario_id" = x$scenario_id[1],
        "var" = vari,
        stringsAsFactors = FALSE)
    )
  }) %>%
  do.call(rbind.data.frame, .)
# Get scenarios
op2 <- op %>%
  left_join(., scen %>%
              select(scenario_id, n, n_t, zeta, Q) %>%
              distinct())
# Estimate of variance parameter estimates
varest <- op2 %>% select(-bias, -MSE, -scenario_id) %>% 
  #filter(zeta == 2 & Q == 0.4 & n == 10 & n_t == 400) %>% 
  mutate(var_est = emp_se ^ 2) %>%
  arrange(desc(emp_se))
library(ggplot2)
ggplot(op2 %>%
         filter(Q == 0.4), aes(x=n, y=bias, linetype=as.factor(n_t), color=as.factor(zeta))) +
  geom_point(size=1.7) +
  geom_hline(yintercept=0, color = "brown", size=1.8) +
  geom_line() +
  facet_wrap(". ~ var", scales="free_y")
```

### Emperical SE

In most cases, large gains are not expected except when $n_t=400$, suggesting that researchers can partially offset the low number of timesteps by adding additional subjects.

```{r}
library(ggplot2)
ggplot(op2 %>%
         filter(Q == 0.4), aes(x=n, y=emp_se, linetype=as.factor(n_t), color=as.factor(zeta))) +
  geom_point(size=1.7) +
  geom_hline(yintercept=0, color = "brown", size=1.8) +
  geom_line() +
  facet_wrap(". ~ var", scales="free_y")
```

### MSE

Interestling, no large gains are observed after $n=40$

```{r}
library(ggplot2)
ggplot(op2 %>%
         filter(Q == 0.1), aes(x=n, y=sqrt(MSE), linetype=as.factor(n_t), color=as.factor(zeta))) +
  geom_point(size=1.7) +
  geom_hline(yintercept=0, color = "brown", size=1.8) +
  geom_line() +
  facet_wrap(". ~ var", scales="free_y")
```

### Coverage

High between-subject variance v. low between-subject variance: much better coverage. Most likely do to bias. See 'bias-corrected coverage' section below.

Nonetheless, there is quite some difference between $zeta=2$ and $zeta=1$ in the sense that the latter has better coverage. Likely because it is more efficient. It is also, in most cases, slightly less biased.

Plot below shows coverage for between-subject variance of $zeta=2$

```{r}
# Variance coverage
var_cov <- lapply(res, function(x) {
  ci <- x$credible_intervals$emiss_varmu_bar
  nams <- names(ci)
  for(idx in seq_along(nams)) {
    names(ci[[idx]]) <- paste0("mean_",
                               c("state1_lower", "state1_upper",
                                 "state2_lower", "state2_upper",
                                 "state3_lower", "state3_upper"))
  }
  y <- do.call(cbind.data.frame, ci)
  colnames(y) <- gsub("\\.", "_", colnames(y))
  y
})
# Bind
var_cov <- do.call(rbind.data.frame, var_cov)
var_cov$iteration_id <- vmean_orig$iteration_id
var_cov2 <- var_cov %>%
  left_join(scen %>%
              select(scenario_id, iteration_id),
            by = "iteration_id")
# Get scenarios
varm <- varm_orig %>%
  left_join(scen %>%
              select(scenario_id, iteration_id),
            by = "iteration_id") %>%
  gather(var, val, -iteration_id, -scenario_id)
# Split by scenario
varm_split <- split(varm, list(varm$scenario_id,varm$var))
# Compute coverage
out_coverage <- varm_split %>%
  map(., function(x) {
    ss <- var_cov2 %>%
      filter(scenario_id == x$scenario_id[1]) %>%
      select(starts_with(x$var[1]))
    ci <- lapply(1:nrow(ss), function(z) unname(unlist(ss[z,])))
    gt <- scen %>% 
      filter(iteration_id == x$iteration_id[1]) %>% 
      select(zeta) %>% 
      pull()
    #gt <- mean(x$val)
    covv <- unname(coverage(ci, gt))
    data.frame(
      iteration_id = x$iteration_id[1],
      scenario_id = x$scenario_id[1],
      var = x$var[1],
      coverage = covv[1]
    )
  })
out_coverage <- do.call(rbind.data.frame, out_coverage)
# Combine with scenarios
out_coverage <- out_coverage %>%
  left_join(scen %>%
              select(scenario_id, n, n_t, Q, zeta) %>%
              distinct(),
            by = "scenario_id")
ggplot(out_coverage %>%
         filter(zeta==2), aes(x=n, y=coverage, linetype=as.factor(n_t), color=as.factor(Q))) +
  geom_point(size=2) +
  geom_line() +
  facet_wrap(". ~ var")
```

In general, coverage is quite good (approx $0.9$) with $n=40$ people. Deteriorates after that.

Plot below shows coverage for $zeta=1$

```{r}
ggplot(out_coverage %>%
         filter(zeta==1), aes(x=n, y=coverage, linetype=as.factor(n_t), color=as.factor(Q))) +
  geom_point(size=2) +
  geom_line() +
  facet_wrap(". ~ var")
```

Plot below shows coverage for $zeta=0.25$

```{r}
ggplot(out_coverage %>%
         filter(zeta==0.25), aes(x=n, y=coverage, linetype=as.factor(n_t), color=as.factor(Q))) +
  geom_point(size=2) +
  geom_line() +
  facet_wrap(". ~ var")
```

As expected, coverage is not very good. This is likely the result of the high bias. The plot below displays bias-corrected coverage:

```{r}
out_coverage <- varm_split %>%
  map(., function(x) {
    ss <- var_cov2 %>%
      filter(scenario_id == x$scenario_id[1]) %>%
      select(starts_with(x$var[1]))
    ci <- lapply(1:nrow(ss), function(z) unname(unlist(ss[z,])))
    gt <- var2 %>%
      filter(scenario_id == x$scenario_id[1],
             var == x$var) %>%
      select(val) %>%
      pull() %>%
      mean()
    #gt <- mean(x$val)
    covv <- unname(coverage(ci, gt))
    data.frame(
      iteration_id = x$iteration_id[1],
      scenario_id = x$scenario_id[1],
      var = x$var[1],
      coverage = covv[1]
    )
  })
out_coverage <- do.call(rbind.data.frame, out_coverage)
# Combine with scenarios
out_coverage <- out_coverage %>%
  left_join(scen %>%
              select(scenario_id, n, n_t, Q, zeta) %>%
              distinct(),
            by = "scenario_id")
ggplot(out_coverage %>%
         filter(zeta==0.25), aes(x=n, y=coverage, linetype=as.factor(n_t), color=as.factor(Q))) +
  geom_point(size=2) +
  geom_line() +
  facet_wrap(". ~ var")
```

Corrected for biased estimates, we observe good coverage. This indicates that the source of the undercoverage is in fact the high bias.

## Transition probabilities

### Bias

Interesting thing here is that the transition probabilities seem:

1. Biased downwards for diagonal entries.
2. Biased upwards for off-diagonal entries.

Between-subject variance has no real effect on this.

Interesting also is that the between-subject variance for TPM does not really affect estimation. As perhaps expected, the number of time steps per subject is more important for getting betting estimates.

However, very small values (e.g. s2 to s1, about 0.003) are always over-estimated.

```{r}
# Transition probabilities
vem <- res %>%
  get("gamma_prob_bar") %>%
  bind_rows()
# Add iteration id
vem$iteration_id <- res %>%
  get("iteration_uid") %>%
  unname() %>%
  unlist()
# Add scenario details
vem2 <- vem %>%
  left_join(scen %>%
              select(iteration_id,
                     scenario_id,
                     n, n_t, zeta, Q)) %>%
  # Gather
  gather(var, val, -iteration_id, -scenario_id,
         -n, -n_t, -Q, -zeta)
# Get ground-truth transition probabilities
library(sleepsimR)
opts <- getOption("sleepsimR_simulate")
tpm <- data.frame(
  "var" = unique(vem2$var),
  "val" = as.vector(opts$gamma_bar)
)
# Compute bias
b <- vem2 %>%
  split(list(vem2$scenario_id, vem2$var)) %>%
  map(function(x) {
    vari <- unique(x$var)
    gt <- tpm %>% filter(var == vari) %>%
      select(val) %>% pull()
    val <- x$val
    bb <- unname(bias(gt, val))
    ese <- unname(emperical_SE(val))
    MSE <- MSE(val, gt)
    return(
      data.frame(
        "bias" = (bb[1] / gt) * 100,
        "emp_se" = ese[1],
        "MSE" = MSE,
        #"MCMC_ERR" = bb[2],
        "scenario_id" = x$scenario_id[1],
        "var" = vari,
        stringsAsFactors = FALSE)
    )
  }) %>%
  do.call(rbind.data.frame, .)
# Merge
op2 <- b %>%
  left_join(scen %>%
              select(scenario_id, n, n_t, Q, zeta) %>%
              distinct(), by="scenario_id")
# Estimate of variance parameter estimates
varest <- op2 %>% select(-bias, -MSE, -scenario_id) %>% 
  #filter(zeta == 2 & Q == 0.4 & n == 10 & n_t == 400) %>% 
  mutate(var_est = emp_se ^ 2) %>%
  arrange(desc(var_est))

library(ggplot2)
ggplot(op2 %>%
         filter(zeta == 0.25), aes(x=n, y=bias, linetype=as.factor(n_t), color=as.factor(Q))) +
  geom_point(size=1.7) +
  geom_line() +
  geom_hline(yintercept=0, color = "brown") +
  facet_wrap(". ~ var", scales="free_y")
```

```{r}
# Plot distribution
op3 <- vem2 %>%
  left_join(scen %>%
              select(scenario_id, n, n_t, zeta, Q) %>%
              distinct()) %>%
  left_join(tpm, by ="var") %>%
  mutate(pbias = ((val.x - val.y)/val.y) * 100)
ggplot(op3 %>%
         filter(Q==0.4, zeta==1, n_t == 800), aes(x=as.factor(n), y=pbias)) +
    geom_boxplot() +
    geom_hline(yintercept=0, color="brown", linetype="dashed") +
    facet_wrap(". ~ var", scales = "free_y")
```

So the bias becomes less pronounced, but it rarely gets to $0$

### Emperical SE

```{r}
ggplot(op2 %>%
         filter(zeta == 2), aes(x=n, y=emp_se, linetype=as.factor(n_t), color=as.factor(Q))) +
  geom_point(size=1.7) +
  geom_line() +
  geom_hline(yintercept=0, color = "brown") +
  facet_wrap(". ~ var", scales="free_y")
```

In all cases, the estimates become more efficient as $n$ grows large. It is interesting that the difference between $400$ timesteps and $1600$ timesteps is relatively small. When $n$ is large, adding more time steps can be useful in obtaining more efficient estimates, but it may not really add much.

### MSE

```{r}
ggplot(op2 %>%
         filter(zeta == 2), aes(x=n, y=sqrt(MSE), linetype=as.factor(n_t), color=as.factor(Q))) +
  geom_point(size=1.7) +
  geom_line() +
  geom_hline(yintercept=0, color = "brown") +
  facet_wrap(". ~ var", scales="free_y")
```

In terms of the MSE, we see that adding more observations surely helps in getting more efficient estimates. Anything beyond $n=40$ is perhaps not really worth it.

### Coverage

In terms of the coverage, we see that the biased estimates lead to pretty bad coverage overall. In most cases, having many observations rather than fewer observations is better (except from s3 to s2 for some reason). You are practically guaranteed to have a coverage of $0$ on all transition probabilities if you only have $400$ observations per subject.

```{r}
# Transition probabilities
tpm_cov <- lapply(res, function(x) {
  ci <- x$credible_intervals$gamma_prob_bar
  io <- c("int_S1toS1_lower", "int_S1toS1_upper",
           "int_S1toS2_lower", "int_S1toS2_upper",
           "int_S1toS3_lower", "int_S1toS3_upper",
           "int_S2toS1_lower", "int_S2toS1_upper",
           "int_S2toS2_lower", "int_S2toS2_upper",
           "int_S2toS3_lower", "int_S2toS3_upper",
           "int_S3toS1_lower", "int_S3toS1_upper",
           "int_S3toS2_lower", "int_S3toS2_upper",
           "int_S3toS3_lower", "int_S3toS3_upper")
  y <- data.frame(ci)
  colnames(y) <- io
  y
})
# Bind
tpm_cov <- do.call(rbind.data.frame, tpm_cov)
tpm_cov$iteration_id <- vmean_orig$iteration_id
tpm_cov <- tpm_cov %>%
  left_join(scen %>%
              select(scenario_id, iteration_id),
            by = "iteration_id")
# Get scenarios
vem2 <- vem %>%
  left_join(scen %>%
              select(scenario_id, iteration_id),
            by = "iteration_id") %>%
  gather(var, val, -iteration_id, -scenario_id)
# Split by scenario
vem2_split <- split(vem2, list(vem2$scenario_id, vem2$var))
# Compute coverage
out_coverage <- vem2_split %>%
  map(., function(x) {
    ss <- tpm_cov %>%
      filter(scenario_id == x$scenario_id[1]) %>%
      select(starts_with(x$var[1]))
    ci <- lapply(1:nrow(ss), function(z) unname(unlist(ss[z,])))
    gt <- tpm %>%
      filter(var == x$var[1]) %>%
      select(val) %>%
      pull()
    #gt <- mean(x$val)
    covv <- unname(coverage(ci, gt))
    data.frame(
      iteration_id = x$iteration_id[1],
      scenario_id = x$scenario_id[1],
      var = x$var[1],
      coverage = covv[1]
    )
  })
out_coverage <- do.call(rbind.data.frame, out_coverage)
# Combine with scenarios
out_coverage <- out_coverage %>%
  left_join(scen %>%
              select(scenario_id, n, n_t, Q, zeta) %>%
              distinct(),
            by = "scenario_id")
ggplot(out_coverage %>%
         filter(zeta==0.5), aes(x=n, y=coverage, linetype=as.factor(n_t), color=as.factor(Q))) +
  geom_point(size=2) +
  geom_line() +
  facet_wrap(". ~ var")
```

### Bias-adjusted coverage

The bias-adjusted coverage indicates that the poor coverage displayed in the above plot is due to the biased estimates, and not some other reason.

```{r}
# Compute coverage
out_coverage <- vem2_split %>%
  map(., function(x) {
    ss <- tpm_cov %>%
      filter(scenario_id == x$scenario_id[1]) %>%
      select(starts_with(x$var[1]))
    ci <- lapply(1:nrow(ss), function(z) unname(unlist(ss[z,])))
    gt <- tpm %>%
      filter(var == x$var[1]) %>%
      select(val) %>%
      pull()
    gt <- mean(x$val)
    covv <- unname(coverage(ci, gt))
    data.frame(
      iteration_id = x$iteration_id[1],
      scenario_id = x$scenario_id[1],
      var = x$var[1],
      coverage = covv[1]
    )
  })
out_coverage <- do.call(rbind.data.frame, out_coverage)
# Combine with scenarios
out_coverage <- out_coverage %>%
  left_join(scen %>%
              select(scenario_id, n, n_t, Q, zeta) %>%
              distinct(),
            by = "scenario_id")
ggplot(out_coverage %>%
         filter(zeta==0.5), aes(x=n, y=coverage, linetype=as.factor(n_t), color=as.factor(Q))) +
  geom_point(size=2) +
  geom_line() +
  facet_wrap(". ~ var")
```

## Recommendations

- Emission distributions:
  * In general, collecting data on more subjects is more important than collecting more data for the same subjects.
  * For high between-subject variance, it is unlikely that more subjects lead to less biased parameter estimates.
  * It is possible to achieve decent parameter estimation even if there is a lot of overlap between state distributions. Using multiple outcome variables in which distributions overlap in slightly different ways ensures that label switching is not a huge problem.
  * Coverage of state-dependent emission distribution means is decent, even when the between-subject variance is high. For high settings, however, it will be smaller than the requisite $90\%$.
  * Lower between-subject variance leads to more precise parameter estimates. This is judged by the MSE and emperical SE. It only suffices to add additional subjects. Adding observations does not help.
  * If random effects are more important for the study than fixed effects, there must be a reasonable amount of variance between subjects, otherwise the variance terms are biased upwards. This is a trade-off between estimating the variance term and the emission distribution means.
  * The coverage of random effects is poor at the lower range, which is due to the high estimation bias. It is decent for higher levels of between-subject variance.
- Transition probabilities:
  * If the self-transition probabilities are large, they are likely to be biased downwards. In general, the coverage of the self-transition probabilities is better than the off-diagonal probabilities.
  * Very small transition probabilities are biased upwards and attain almost no coverage. 
  * If a decent estimation is required of the transition probabilities, then adding more observations for each subject is more important than adding more subjects (beyond $n=40$). Researchers can be almost certain that the transition probabilities cannot be estimated properly without many timesteps for each subject.
  * For reasonable coverage on the diagonal transition probabilities, researchers need at least $1.600$ occasions.
  * There is little to no difference between the variance settings. Even with large between-subject differences on the transition probabilities the performance is about the saame.